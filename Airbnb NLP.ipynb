{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UC Davis | STA160 Group 7 | Midterm Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checklist\n",
    "\n",
    "**Data Cleaning**\n",
    "- Take out too expensive prices  **DONE**\n",
    "    - look at the info when doing this \n",
    "- Take out zero avalibility **DONE**\n",
    "    -   check other stuff\n",
    "-   Any reviews in 2019\n",
    "-   look into cleaning the listings with 0 reviews **DONE**\n",
    "\n",
    "**EDA**\n",
    "- Basic Graphs visiualizing \n",
    "    - prices \n",
    "    - neghborhoods \n",
    "    - density/heat map\n",
    "- Ivan \n",
    "- Rishi\n",
    "\n",
    "**NLP**\n",
    "- Cleaning on name column\n",
    "- Vishnu\n",
    "- ADI\n",
    "- Go Into Graph\n",
    "\n",
    "**Write the interpretation**\n",
    "- Ivan \n",
    "- Rishi\n",
    "\n",
    "\n",
    "**Given Time** \n",
    "- Explore other research questions\n",
    "    - such as housing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Info\n",
    "\n",
    "UC DAVIS STA 160 Group 7\n",
    "\n",
    "Group Members \n",
    "\n",
    "- Vishnu Rangiah, 916562849, vrrangiah@ucdavis.edu\n",
    "- Aditya Kallepalli, 915079375, arkallepalli@ucdavis.edu\n",
    "- Rishi Bhuva, 915218518, rdbhuva@ucdavis.edu\n",
    "- Ivan Yang, 915463046 igyang@ucdavis.edu\n",
    "\n",
    "\n",
    "## Abstract\n",
    "\n",
    "- 200 word abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this report we will be analyzing Airbnb data from the Airbnb open new york city 2019 data set. \n",
    "\n",
    "\n",
    "Airbnb is a platform that operates an online marketplace for lodging, primarily homestays for vacation rentals, and tourism activities. This platform is accessible through website or mobile app. \n",
    "\n",
    "The data we are currently analyzing comes from : https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data\n",
    "\n",
    "This dataset describes listing activity and metrics in New York City, New York for 2019. Through this dataset, we can get information regarding hosts, geographical availability and necessary metrics needed to make predictions and draw conclusions. \n",
    "\n",
    "We will be performing our analysis using Python with the packages xxxxxx . In Section II, we will be cleaning, and preforming exploratory analysis on the data. Then we will explore the relationship between the names of each listing and the popularity of the listing using Natural Language Processing Methods in Section III.\n",
    "\n",
    "[//]: attachment:./# (We will be cleaning, and preforming exploratory analysis on the data. \n",
    "Then we will explore the realtionship between the Names of each listing and the popularity of the listing using Natural Language Processing Methods.)\n",
    "\n",
    "\n",
    "#### NEXT STEPS: EDA, NLP, MODELS, CONCLUSION "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import plotly.offline as py\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "import more_itertools as more\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "df = pd.read_csv('Airbnb_NYC_2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dataframe_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    "\n",
    "# Our original Dataset\n",
    "# Explain dataset, possibly provide graphs to display data more properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we find that our dataset contains 48,895 rows which represent indivual listings in New York. We are also provided with 16 different columns which contain necessary factors needed for exploratory data analysis. \n",
    "\n",
    "We can see these different columns below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.columns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No. of Variables + Entries, Variable Names, and Data Types:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that our dataframe consists of 48,895 rows and 16 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Types of Each Column\n",
    "\n",
    "A brief analysis as to what data type each column is can be seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning  \n",
    "\n",
    "- Find outliers \n",
    "- Get rid of listing that are too expensive \n",
    "    - Why is the price so high: super bowl?\n",
    "- Get rid of listing that do not have avalibility for 2019\n",
    "- Possibly find potential negative and NA values \n",
    "\n",
    "- The Airbnb calendar for a listing does not differentiate between a booked night vs an unavailable night, therefore these bookings have been counted as \"unavailable\". This serves to understate the Availability metric because popular listings will be \"booked\" rather than being \"blacked out\" by a host.\n",
    "\n",
    "- avaliblity_365 :  number of days the property is avalibly for rent \n",
    "    - Data shows alot of properties of 0 days avalible \n",
    "        - Why was it 0: \n",
    "            - host didnt want to rent or host took off listing but it was still on sight \n",
    "            - airbnb suspended listing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first line of action when analyzing this data is to properly clean the dataset. Having clean data will provide us with the highest quality of information needed and therefore will provide us with the most accurate predictions and correlations. Our process for cleaning the dataset can be seen below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first check the number of rows with missing values. We will then remove such rows depending on the missing variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are 4 variables with missing values. However, we've determined that all of these missing variables do not greatly affect our NLP modelling. Therefore, we've decided to keep these listings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, while combing through our main dataframe, we discovered that there are a number of listings with 0 available days to be rented as well as 0 total reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.availability_365 == 0] # Elaborate on why zero availabilty would occur/ show possible plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe above consists of listings with zero availability. We chose to extract these listings from the dataframe we will analyze, because zero availablity means that the listings is not available at all. Therefore, this data would be irrelevant as we are want to perform analysis on listings that have availablity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.number_of_reviews == 0] # Elaborate on why some number of reviews would be zero / show possible plots\n",
    "\n",
    "#We will remove entries with 0 days of availabilities and 0 reviews.\n",
    "#We believe that these are faulty listings that were not completely removed from the AirBnB server. \n",
    "#Most of these listings were not available under the main English website but were viewable in different languages such as Spanish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe above consists of listings with zero number of reviews. We chose to extract these values from the data we want to analyze, because number of reviews is a factor that can tell us a lot about the popularity of the listing and is an important component to our Natural Language Processing Method which we will perform. Therefore, listings with zero number of reviews would be irrelevant data and not needed for our type of analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Due to the drawbacks discussed regarding 0 days of availabilities and 0 number of reviews, it would be best to simply remove these entries in the dataset we want to work with. Below, we properly remove these listings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df[df.availability_365 != 0] # Remove entries with 0 days of availabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.number_of_reviews != 0] # Remove entries with 0 number of reviews\n",
    "#df = df[df.price != 0] # Remove entries that consist of $0 price\n",
    "df.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we can see the dimensions of our reduced dataframe, excluding values of avaiablity and number of reviews being 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will examine the price range of all AirBnB listings by separating these listings into quartiles to get a general insight for this column alone. Along with the 25, 50, 75 % quartile calculations, we can also get information such as the mean, standard deviation and the minimum and maximum value of our price category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "quartiles = df[[\"price\"]].describe()\n",
    "quartiles\n",
    "\n",
    "#From the initial quartile calculation, we see that there are listings with $0 as price. \n",
    "#Inspecting listings with `price` = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As observed in our quartile table above, there are interestingly listings that are $0. Our next course of action would be to pull up these listings and do further research.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.loc[lambda df: df.price == 0]  # Possibly explain why some listings in our data would be $0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After multiple online searches of these listings, we've come up with mixed results of these listings' robustness. Listings such as Kimberly's seem to have been removed while all three of Adeyemi's listings are still up on AirBnB and running in 2021. Additionally, there isn't any way that we can revisit AirBnB's 2019 websites to verify these listings.\n",
    "\n",
    "One possible reason for these $0 prices may have been a deliberate act by the hosts to temporarily remove the listing from the AirBnB market when these listings were webscraped. The additional fact that among these 0 USD listings, 3 and 2 of them belong to the same host may further indicate that it is a host-activated anomaly. \n",
    "\n",
    "Either way, we will remove these listings just to minimize any possibility of accruing errors based on unknown anomalies.\n",
    "\n",
    "Below, is the process of removing entries with the price = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.price != 0] # Remove listings that have $0 price.\n",
    "quartiles = df[[\"price\"]].describe() # See the statistics of price to further analyze\n",
    "quartiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we can see our new summary statistics with $0 of price removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(x = df['price'], vert = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the data is quite skewed with multiple extremely high prices heavily affecting the data. Further research also shows that some of these data points are faulty listings, such as a 2012 Superbowl private room listing.\n",
    "\n",
    "We will use the 1.5 Interquartile Range Test to determine outliers and store these listings in a different dataframe for further data analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the 1.5 Interquartile Range Test to determine outliers\n",
    "\n",
    "IQR = np.percentile(df.price, 75)-np.percentile(df.price, 25)\n",
    "lower_limit = np.percentile(df.price, 25)- 1.5*IQR\n",
    "upper_limit = np.percentile(df.price, 75) + 1.5*IQR\n",
    "print(lower_limit, upper_limit) \n",
    "# Calculations for outliers within the price entry of data, used to deem what is considered \"too expensive.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, through the IQR test, we can calculate for outliers within the price entry of data. We can see that anything above the price of 332.5 dollars would be considered too expensive, and therefore would be an outlier.  Since there technically cannot be any negative values for prices, we will fix the lower limit at 10 dollars which is the minimum value as seen in our quartile table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df\n",
    "\n",
    "#Since there technically cannot be any negative values for prices, we will fix the lower limit at $10 which is the minimum value as seen in our quartile table.\n",
    "\n",
    "outliersdf = df.loc[lambda df: df.price > upper_limit]\n",
    "outliersdf # Dataset of what is considered too expensive, conduct plots to see analysis.\n",
    "# Possibly explain why these listings would be considered expensive.\n",
    "\n",
    "# could also use df.loc[lambda df: df.price >=6000]\n",
    "\n",
    "#Do futher processing here later \n",
    "#df_1 = df.loc[lambda df: df.price <= 6000] # df without outliers\n",
    "#df_1\n",
    "\n",
    "outliersdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidied Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we have cut all outliers and properly cleaned the dataset and can move on to exploratory data analysis. We can also see the dimensions of our cleaned up dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidydf = df.loc[lambda df: df.price <= upper_limit]\n",
    "tidydf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,12))\n",
    "sns.heatmap(tidydf.corr(),annot=True) # Explain corr plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see a proper correlation matrix between the variables within our dataset. According to this plot, we can see that the more lighter the color, the higher the correlation between each variable. Vice versa, we can see that the darker the number, the lower the correlation. From this plot, we can immediately see a high correlation of 0.6 between id and host_id. Along with this, it can be seen that number of reviews and reviews per month also have a higher correlation than others at 0.49. Along with positive correlations, we can also see negative correlations with id and number of reviews at -0.45 and with id and reviews per month at -0.45. We can also see a negative correlation between price and longitude at -0.3, although a positive correlation of 0.051 between price and latitude. \n",
    "\n",
    "As id is a specific tag per listing, host_id is also a specific tag per host. Therefore, this positive correlation does seem to make sense as some hosts would have multiple listings under their name. Number of reviews and reviews per month also tend to have a positive correlation as each number of review contributes to a review per month and vice versa, therefore these two variables seem to have a direct relationship. \n",
    "\n",
    "The lowest correlation we can see in this plot can be seen between both variables of reviews and id. The large negative correlation between them does seem accurate, as each id is specific and randomly generated and does not relate at all to reviews at all. Another negative correlation that was interesting comes from price and longitude. As longitude tends to be west and east, it can be seen that pricein New York does not tend to have any sort of relationship with those specific directions. Although, price and latitude show a somewhat positive correlation, meaning that price is more correlated with listings that range from the north and south rather than the west and east. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do we need this entire plot or plot boxplots for specific columns?\n",
    "plt.figure(figsize = (12,12))\n",
    "tidydf.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs\n",
    "\n",
    "- Price (whole dataset, by borough, by types of room) **DONE**\n",
    "- Minimum Nights (whole dataset, by borough) **DONE**\n",
    "- Types of Room (whole dataaset, by borough) **DONE**\n",
    "- Reviews per month (whole dataset, by borough) **DONE**\n",
    "- availability 365 (whole dataset, by borough)\n",
    "\n",
    "\n",
    "### If we have time \n",
    "- Some sort of graphs and explaination of popularity \n",
    "- recommended by prof: roc curve\n",
    "\n",
    "### Extra\n",
    "- heat map \n",
    "- box plot \n",
    "- how much people are making \n",
    "- which hosts make the most\n",
    "- observe which neighborhoods have certain amount of avalibilty "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will plot some graphs with our tidied dataset to gain further insight, as well as to visualize any more abnormal data points for further cleaning. We will not be subsetting these outliers from the main tidied dataset as these variables will not be our main focus for our NLP analysis. However, we will generate graphs of these outliers for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\")\n",
    "sns.countplot(y = 'room_type', data= tidydf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that most listings are either private rooms or the entirety of the property. This indicates that most AirBnB customers may be looking for private accomodation rather than shared spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(y = 'neighbourhood_group', data= tidydf).set_title(\"Listings by Borough\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most listings are either in the boroughs of Brooklyn or Manhattan. This is logical given that these 2 boroughs contain the most attractions and activites in New York City. Staten Island has the lowest number of listings, which may be due to its geographical location being relatively inaccessible compared to the other boroughs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn = df['neighbourhood'].value_counts()\n",
    "sns.countplot(y='neighbourhood',data=tidydf, order=dfn.iloc[:10].index).set_title('Top 10 Neighborhoods of Listings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 4 neighborhoods of AirBnB listings are all in Brooklyn, with Bedford-Stuyvesant being the top neighborhood by quite a margin. The 5th to 8th most popular neighborhoods are in Manhattan. This may be due to affordability of Brooklyn compared to Manhattan. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Density Map of All Listings According to Price\n",
    "\n",
    "<https://geopandas.org/gallery/create_geopandas_from_pandas.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install contextily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gdf = gpd.GeoDataFrame(\n",
    "    #tidydf, geometry=gpd.points_from_xy(tidydf.longitude, tidydf.latitude))\n",
    "\n",
    "#ax = gdf.plot(figsize=(15, 15), alpha=0.5, edgecolor='k')\n",
    "#ctx.add_basemap(ax, zoom = 18)\n",
    "#ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import contextily as ctx\n",
    "\n",
    "plt.figure(figsize=(15,30))\n",
    "sns_map = sns.scatterplot(x='longitude', y='latitude', hue='price',s=20, data=tidydf)\n",
    "ctx.add_basemap(sns_map, crs = 'EPSG:4326', source=ctx.providers.CartoDB.Positron)\n",
    "sns_map.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the density map above, we can see that most of the listings are concentrated around the boroughs of Brooklyn and Manhattan. We can also observe that Manhattan has a higher density of expensive listings, and apart from standout anomalies, prices tend to be lower as listings become further from Manhattan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphs in relation to price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.boxplot(x='price', data=df)\n",
    "px.histogram(tidydf, x = 'price', title = 'Price Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the histogram above, we can see a left-skewed unimodal distribution. A majority of the listings hover around a price between 35 to 130 USD. However, there are significant spikes at 150, 200, 250, and 300 USD bins, as well as smaller spikes at 175, 225, 275, and 325 USD bins. We believe that this may be caused by human psychology where hosts may round their prices to the nearest 25 or 50 dollar for a more \"aesthetically pleasing\" price number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#by room type and borough\n",
    "sns.catplot(x='neighbourhood_group', y='price', data = tidydf, hue = 'room_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an obvious trend that shared rooms are the cheapest option in all boroughs, followed by private room and entire property. Additionally, Staten Island and the Bronx have a lot less listings above 150USD compared to Brooklyn and Manhattan. Additionally, as seen in our density map, Manhattan has the highest density of expensive listings compared to other boroughs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,30))\n",
    "sns_map_out = sns.scatterplot(x='longitude', y='latitude', hue='price',s=20, data=outliersdf)\n",
    "ctx.add_basemap(sns_map_out, crs = 'EPSG:4326', source=ctx.providers.CartoDB.Positron)\n",
    "sns_map_out.set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning about outlying prices according to room type\n",
    "sns.catplot(x='room_type', y='price', data = outliersdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the outlying expensive listings are either for entire properties or private rooms. Interestingly, the most expensive listing is a 10000USD private room in Queens. This is listing is now defunct, but we can infer that it might be a long term rental situation as the listing also indicates a minimum night requirement of 100 nights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning about outliers according to neighborhood and room type\n",
    "sns.catplot(x='neighbourhood_group', y='price', data = outliersdf, hue = 'room_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphs in relation to minimum nights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.boxplot(x='minimum_nights', data=df)\n",
    "px.histogram(tidydf, x = 'minimum_nights', title = 'Minimum Nights Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(tidydf.minimum_nights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most listings in 2019 NYC have a minimum night requirement between 1 to 7 nights, with an average minimum night requirement of 6.64 and 2 nights being the hightest. There is an apparent spike of listings with a minimum night requirement of 30 nights.\n",
    "\n",
    "There seems to be multiple outliers, with one as extreme as 1250 minimum nights. This wide range of minimum night requirements allude to the existence of not just long term vacation rental properties but also potentially long term residence.\n",
    "\n",
    "We will determine their outliers again using the 1.5 IQR test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the 1.5 Interquartile Range Test to determine outliers\n",
    "\n",
    "IQR_mn = np.percentile(tidydf.minimum_nights, 75)-np.percentile(tidydf.minimum_nights, 25)\n",
    "lower_limit_mn = np.percentile(tidydf.minimum_nights, 25)- 1.5*IQR\n",
    "upper_limit_mn = np.percentile(tidydf.minimum_nights, 75) + 1.5*IQR\n",
    "print(lower_limit_mn, upper_limit_mn) \n",
    "# Calculations for outliers within the price entry of data, used to deem what is considered \"too expensive.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since there technically cannot be any negative values for min nights, we will fix the lower limit at 0.\n",
    "\n",
    "outliersdf_mn = tidydf.loc[lambda tidydf: tidydf.minimum_nights > upper_limit_mn]\n",
    "outliersdf_mn # Dataset of what is considered too expensive, conduct plots to see analysis.\n",
    "# Possibly explain why these listings would be considered expensive.\n",
    "\n",
    "# could also use df.loc[lambda df: df.price >=6000]\n",
    "\n",
    "#Do futher processing here later \n",
    "#df_1 = df.loc[lambda df: df.price <= 6000] # df without outliers\n",
    "#df_1\n",
    "\n",
    "outliersdf_mn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidydf_mn = tidydf.loc[lambda tidydf: tidydf.minimum_nights <= upper_limit_mn]\n",
    "tidydf_mn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x='neighbourhood_group', y='minimum_nights', data = tidydf_mn, hue='room_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph, it seems like Staten Island and the Bronx are not popular boroughs for long term rentals longer than 30 days. Further investigation of the few orange anomalies in the Bronx reveal that they are all from the same host, Sasha (Host ID: 2988712), who seems to have multiple 90-day listings around Claremont Village and Mount Hope in the Bronx.\n",
    "\n",
    "Most longer term rentals in Brooklyn, Manhattan, and Queens are of entire properties. Interesting anomalies include the 90 day minimum night requirement listings for a shared room in Manhattan. Further investigation shows that these listings (Host IDs: 21628183 and 23184420) are both from LaGuardia Houses Public Housing Development looking for long term housemates. We believe that these listings are a result of hosts trying to bypass New York's legal barrier of subletting public housing for additional income, adding onto the high accessibility of AirBnB for anyone to put out a listing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x='neighbourhood_group', y='minimum_nights', data = outliersdf_mn, hue='room_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, most extreme values of minimum night requirement are from listings in Manhattan and Brooklyn, with Queens and the Bronx each only having 1 listing and Staten Island not having any. Both listings in Queens and the Bronx seem to be long term rental leasings, just like most of the other leasings in this plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph by Reviews Per Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(tidydf, x = 'reviews_per_month', title = 'Reviews Per Month Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(tidydf.reviews_per_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of reviews are heavily skewed to the left with listings getting an average of 1.83 reviews per month. There are obvious outliers, with a listing getting as many as 58.5 average reviews per month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x='neighbourhood_group', y='reviews_per_month', data = tidydf, hue='room_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that most boroughs have a similar pattern of reviews per month, with 2 anomalies in Manhattan. Further analysis reveals that both listings (ID: 32678719 and 32678720) are by the same host Row NYC (Host ID: 244361589) which is a hotel in New York City. They may have more reviews as they may be more attractive to customers as an established hotel chain. Additionally, they may have multiple rooms available under the same listing, which would explain why they have listings getting 58.5 reviews in a span of 30 or 31 days. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphs by Availabile Days out of 365 Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df, x = 'availability_365', title = 'Distribution of Available Days out of 365 days')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a bimodal distribution with spkies at both ends. This tells us that on one extreme, most NYC hosts were not leasing out their place during 2019, or maybe for a duration not more than a week. On the other extreme, there were hosts that had listings available for almost the entire year of 2019. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x='neighbourhood_group', y='availability_365', data = tidydf, hue='room_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All boroughs had availabilities rangine from 0 to 365, with not much of a differentiation between room types. It is interesting to note that a majority of Staten Island listings were available for more days of the year compared to other boroughs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP\n",
    "\n",
    "- Use nlp libraries to group names \n",
    "- find correlation to popularity maybe price\n",
    "\n",
    "- Word differences from different room types \n",
    "- popularity: some how combine the popularity scores\n",
    "- pricing \n",
    "- neighborhoods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be exploring the name feature of the Airbnb dataset in depth using NLP techniques. With the use of the popular NLP library spacy we are able to clean the listing names and extract relevant information from them such as the most frequnctly used words in listings as well as non english symbols used in the listings. \n",
    "\n",
    "In our analysis of the names of the listings we provide visualizations to show the relationship of the name listings across price subgroups and popularity subgroups which we equated to be the number of reviews per month. \n",
    "\n",
    "Additionally we explore the unique listing names which contain non english symbols and plot their locations to see if there is any correlation between the language used in the listing and the listing location. This correlation may be present in listing names which contain chinese characters and are potentially located in china town."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r_low = df[df['reviews_per_month'].between(0, 0.45)] #25%\n",
    "df_r_mid = df[df['reviews_per_month'].between(0.46, 1.23)] #50%\n",
    "df_r_high = df[df['reviews_per_month'].between(1.24, 2.68)] #75%\n",
    "df_r_extra = df[df['reviews_per_month'].between(2.69, 70)] #75%\n",
    "\n",
    "#Need better way of splitting the dataset based on quantiles maybe have list of quantiles and create functions \n",
    "\n",
    "#show the amount of listings with quantiles \n",
    "\n",
    "df_p_low = df[df['price'].between(0, 70)] #25%\n",
    "df_p_mid = df[df['price'].between(70.01, 109)] #50%\n",
    "df_p_high = df[df['price'].between(109.01, 175)] #75%\n",
    "df_p_extra = df[df['price'].between(175, 9999)] #100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r_low.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p_low.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quartiles_r = df[[\"reviews_per_month\"]].describe()\n",
    "\n",
    "quartiles_p = df[[\"price\"]].describe()\n",
    "\n",
    "print(df_r_low.shape, '\\n\\n'\n",
    ", quartiles_r, '\\n\\n', quartiles_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word cloud of all not cleaned names \n",
    "\n",
    "df_nc = pd.read_csv('Airbnb_NYC_2019.csv')\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "corpus = list(df_nc[\"name\"].values)\n",
    "clean_named = (' '.join(w for w in corpus if isinstance(w, str) ))\n",
    "\n",
    "wordcloud = WordCloud(width = 1200, height = 600,\n",
    "                background_color ='white',\n",
    "                min_font_size = 10).generate(clean_named)\n",
    "\n",
    "# plot the WordCloud image                       \n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Why are we using the cleaned dataset for names***\n",
    "\n",
    "Reason: For the most part it takes out irrelevant listing which potenitally were not listing active in 2019. In terms of the focus of this report we ideally aim to see active listing in 2019 and compare the relationship between subgroups of price and popularity via number of review per month. \n",
    "\n",
    "Reason: we only use the cleaned dataset because we want to only look at names that resemble the listing for airbnb in newyork during 2019. The cleaned data removes all the data that is invalid.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore chinatown name find locations of the chinese names\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #COUNTS MOST POPULAR NAMES\n",
    "# from collections import Counter\n",
    "\n",
    "# #clean_named\n",
    "\n",
    "# # split() returns list of all the words in the string\n",
    "# split_it = clean_named.split()\n",
    "  \n",
    "# # Pass the split_it list to instance of Counter class.\n",
    "# Counter = Counter(split_it)\n",
    "  \n",
    "# # most_common() produces k frequently encountered\n",
    "# # input values and their respective counts.\n",
    "# most_occur = Counter.most_common(30)\n",
    "  \n",
    "# print(most_occur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RANDOM CODE GOT FROM CLASS\n",
    "#dividing cars into 3 categories by weight\n",
    "# min_price = min(df['weight'])\n",
    "# max_price = max(df['weight'])\n",
    "# per_bin = (max_wt - min_wt)/3\n",
    "\n",
    "# bin1_start = min(df['weight'])\n",
    "# bin1_end = bin1_start + per_bin\n",
    "# bin2_end = bin1_end + per_bin\n",
    "\n",
    "# def label_wt(row):\n",
    "#     if bin1_start <= row['weight'] and row['weight'] <= bin1_end:\n",
    "#         return 'light'\n",
    "#     elif row['weight'] <= bin2_end:\n",
    "#         return 'medium'\n",
    "#     else:\n",
    "#         return 'heavy'\n",
    "# df['weight_categ'] = df.apply(lambda row: label_wt(row),axis=1)\n",
    "# df.head()\n",
    "# print(\"Thresholds: \\nBin 1 = \",bin1_end,\"\\nBin 2 = \",bin2_end,\"\\nBin 3 = \",max_wt,\"\\nSize of each bin = \",per_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer()\n",
    "# X = vectorizer.fit_transform(clean_names)\n",
    "\n",
    "# threshold = 0.2\n",
    "\n",
    "# for x in range(0,X.shape[0]):\n",
    "#   for y in range(x,X.shape[0]):\n",
    "#     if x!=y:\n",
    "#       if cosine_similarity(X[x],X[y])>threshold:\n",
    "#         print(df_1[\"name\"][x],\":\",corpus[x])\n",
    "#         #print(df_1[\"name\"][y],\":\",corpus[y])\n",
    "#         print(\"Cosine similarity:\",cosine_similarity(X[x],X[y]))\n",
    "#         print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Puncuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a data frame that has only id and names.\n",
    "\n",
    "#spacy import \n",
    "from spacy.lang.en import English\n",
    "import spacy\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dfs = [df_r_low, df_r_mid, df_r_high, df_r_extra]\n",
    "\n",
    "names_c = [] #list of all the names as strings\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "nlp.Defaults.stop_words |= {\"room\", \"bedroom\", \"apartment\", \"apt\"} #added stop words \n",
    "\n",
    "for i in dfs: \n",
    "\n",
    "    names_low = i[\"name\"].values\n",
    "\n",
    "    clean_named = (' '.join(str(n).lower() for n in names_low) )\n",
    "\n",
    "    name = nlp(clean_named) #tokinzed name\n",
    "\n",
    "    filtered_name = []\n",
    "\n",
    "    # filtering name from stop words\n",
    "    for word in name:\n",
    "        if word.is_stop==False and word.is_punct==False: #takes out punctuation \n",
    "            filtered_name.append(word)\n",
    "\n",
    "    name_clean = ' '.join([str(w).lower() for w in filtered_name])\n",
    "\n",
    "    names_c.append(name_clean)\n",
    "\n",
    "#find most frequent words \n",
    "names_c\n",
    "#create histogram plots and two way histogram plots \n",
    "# from collections import Counter\n",
    "\n",
    "# def plot_names(names): \n",
    "#     split_it = names.split()\n",
    "#     counts = Counter(split_it)\n",
    "#     # most_common() produces k frequently encountered\n",
    "#     # input values and their respective counts.\n",
    "#     most_occur = counts.most_common(20)\n",
    "#     most_df = pd.DataFrame(most_occur, columns =['Words', 'Freq'])\n",
    "#     plt.figure()\n",
    "#     sns.barplot(y=\"Words\", x=\"Freq\", data=most_df)\n",
    "\n",
    "\n",
    "# #CREATES THE PLOTS\n",
    "# for n in names_c: \n",
    "#     plot_names(n)\n",
    "\n",
    "\n",
    "#WORDS CLOUDS\n",
    "\n",
    "# wordcloud = WordCloud(width = 1200, height = 600,\n",
    "#                 background_color ='white',\n",
    "#                 min_font_size = 10).generate(name_clean)\n",
    "\n",
    "# # plot the WordCloud image                       \n",
    "# plt.figure(figsize = (8, 8), facecolor = None)\n",
    "# plt.imshow(wordcloud)\n",
    "# plt.axis(\"off\")\n",
    "# plt.tight_layout(pad = 0)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_p = [df_p_low, df_p_mid, df_p_high, df_p_extra]\n",
    "\n",
    "names_cp = [] #list of all the names as strings\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "nlp.Defaults.stop_words |= {\"room\", \"bedroom\", \"apartment\", \"apt\"} #added stop words \n",
    "\n",
    "for i in dfs_p:\n",
    "\n",
    "    names_low = i[\"name\"].values\n",
    "\n",
    "    clean_named = (' '.join(str(n).lower() for n in names_low) )\n",
    "\n",
    "    name = nlp(clean_named) #tokinzed name\n",
    "\n",
    "    filtered_name = []\n",
    "\n",
    "    # filtering name from stop words\n",
    "    for word in name:\n",
    "        if word.is_stop==False and word.is_punct==False: #takes out punctuation \n",
    "            filtered_name.append(word)\n",
    "\n",
    "    name_clean = ' '.join([str(w).lower() for w in filtered_name])\n",
    "\n",
    "    names_cp.append(name_clean)\n",
    "\n",
    "    #find most frequent words \n",
    "\n",
    "names_cp\n",
    "# #create histogram plots and two way histogram plots \n",
    "# from collections import Counter\n",
    "\n",
    "lowp = names_cp[0].split()\n",
    "countlowp = Counter(lowp)\n",
    "countlowp50 = countlowp.most_common(50)\n",
    "lowpdf = pd.DataFrame(countlowp50, columns =['Words', 'Freq'])\n",
    "\n",
    "midp = names_cp[1].split()\n",
    "countmidp = Counter(midp) \n",
    "countmidp50 = countlowp.most_common(50)\n",
    "midpdf = pd.DataFrame(countmidp50, columns =['Words', 'Freq'])\n",
    "\n",
    "highp = names_cp[2].split()\n",
    "counthighp = Counter(highp)\n",
    "counthighp50 = counthighp.most_common(50)\n",
    "highpdf = pd.DataFrame(counthighp50, columns =['Words', 'Freq']) \n",
    "\n",
    "extp = names_cp[3].split()\n",
    "countextp = Counter(extp) \n",
    "countextp50 = countextp.most_common(50)\n",
    "extpdf = pd.DataFrame(countextp50, columns =['Words', 'Freq'])\n",
    "\n",
    "asd1 = pd.merge(lowdf,midpdf,on='Words',how='outer')\n",
    "asd2 = pd.merge(asd1,highpdf,on='Words',how='outer')\n",
    "asd3 = pd.merge(asd2,extpdf,on='Words',how='outer')\n",
    "\n",
    "asd3\n",
    "\n",
    "\n",
    "# def plot_names_perc(names): \n",
    "#     split_it = names.split()\n",
    "#     counts = Counter(split_it)\n",
    "#     # most_common() produces k frequently encountered\n",
    "#     # input values and their respective counts.\n",
    "#     most_occur = counts.most_common(20)\n",
    "#     most_df = pd.DataFrame(most_occur, columns =['Words', 'Freq'])\n",
    "#     plt.figure()\n",
    "#     sns.barplot(y=\"Words\", x=\"Freq\", data=most_df)\n",
    "\n",
    "# # for n in names_c: \n",
    "# #     plot_names_perc(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFDIF , bag of words\n",
    "\n",
    "#Parts of speech \n",
    "\n",
    "#Detecting language\n",
    "\n",
    "#Plotting on map where the non english listing are \n",
    "\n",
    "#need to get the two way histogram for that\n",
    "\n",
    "# Number of review, reviews per month, avalibilty 365. \n",
    "### How to develope a score for  popularity. \n",
    "\n",
    "#Default a reviews per month "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other way to clean the names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_names = []\n",
    "\n",
    "\n",
    "for i in list(df_r_extra[\"name\"].values):\n",
    "    text_tokens = word_tokenize(str(i).lower())\n",
    "    clean_toks = [word for word in text_tokens if word not in nlp.Defaults.stop_words and word not in string.punctuation]\n",
    "    i = ' '.join([str(n) for n in clean_toks])\n",
    "    cleaned_names.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "import scipy.sparse as sp\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "num_partitions = num_cores-2 # I like to leave some cores for other\n",
    "#processes\n",
    "print(num_partitions)\n",
    "\n",
    "def parallelize_dataframe(df, func):\n",
    "    a = np.array_split(df, num_partitions)\n",
    "    del df\n",
    "    pool = Pool(num_cores)\n",
    "    #df = pd.concat(pool.map(func, [a,b,c,d,e]))\n",
    "    df = sp.vstack(pool.map(func, a), format='csr')\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def test_func(data):\n",
    "    #print(\"Process working on: \",data)\n",
    "    tfidf_matrix = TfidfVectorizer.transform(data[\"text\"])\n",
    "    #return pd.DataFrame(tfidf_matrix.toarray())\n",
    "    return tfidf_matrix\n",
    "\n",
    "#df = pd.DataFrame({'col': [0,1,2,3,4,5,6,7,8,9]})\n",
    "#df =  data_pd\n",
    "tf_idf = parallelize_dataframe(cleaned_names, test_func)\n",
    "\n",
    "\n",
    "\n",
    "#tf_idf = TfidfVectorizer().fit_transform(cleaned_names)\n",
    "\n",
    "# Use .A to display a sparse matrix.\n",
    "(tf_idf * tf_idf.T).A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "docs_df = pd.DataFrame(TextBlob(d).word_counts for d in cleaned_names).T\n",
    "docs_df = docs_df.fillna(0)\n",
    "docs_df.head()\n",
    "\n",
    "tf = docs_df / docs_df.sum()\n",
    "tf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding non alphabet characters \n",
    "\n",
    "noneng = []\n",
    "\n",
    "\n",
    "for i in list(df[\"name\"].values): \n",
    "    a = str(i)\n",
    "    if a.isascii():\n",
    "        continue\n",
    "    else: \n",
    "        noneng.append(i)\n",
    "  \n",
    "\n",
    "len(noneng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noneng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "chineseLists = []\n",
    "\n",
    "def isChinese(s):\n",
    "    s = str(s)\n",
    "    if len(re.findall(r'[\\u4e00-\\u9fff]+', s))>0:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "for i in noneng: \n",
    "    a = str(i)\n",
    "    if isChinese(a): chineseLists.append(i)\n",
    "\n",
    "chineseLists\n",
    "\n",
    "\n",
    "# sample = '2 br close to everything'\n",
    "# re.findall(r'[\\u4e00-\\u9fff]+', sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emoji import UNICODE_EMOJI\n",
    "\n",
    "emojiLists = []\n",
    "\n",
    "def is_emoji(s):\n",
    "    s = str(s)\n",
    "    count = 0\n",
    "    for emoji in UNICODE_EMOJI:\n",
    "        count += s.count(emoji)\n",
    "        if count > 1:\n",
    "            return False\n",
    "    return bool(count)\n",
    "\n",
    "\n",
    "for i in noneng: \n",
    "    a = str(i)\n",
    "    if is_emoji(a): emojiLists.append(i)\n",
    "\n",
    "emojiLists\n",
    "\n",
    "\n",
    "\n",
    "# sample = '2 br close to everything'\n",
    "# re.findall(r'[\\u4e00-\\u9fff]+', sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_index = []\n",
    "emoji_index = []\n",
    "\n",
    "for i in list(df[\"name\"].values): \n",
    "    a = str(i)\n",
    "    if is_emoji(a) and not a.isascii() :\n",
    "        emoji_index.append(True)\n",
    "    else: \n",
    "        emoji_index.append(False)\n",
    "\n",
    "for i in list(df[\"name\"].values): \n",
    "    a = str(i)\n",
    "    if isChinese(a) and not a.isascii() :\n",
    "        chinese_index.append(True)\n",
    "    else: \n",
    "        chinese_index.append(False)\n",
    "\n",
    "df_noneng = df[chinese_index]\n",
    "df_emoji = df[emoji_index] #logical indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Density Map of Listings with Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,30))\n",
    "sns_map_emoji = sns.scatterplot(x='longitude', y='latitude',s=20, data=df_emoji)\n",
    "ctx.add_basemap(sns_map_emoji, crs = 'EPSG:4326', source=ctx.providers.CartoDB.Positron)\n",
    "sns_map_emoji.set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"t #**3131 #s\"\n",
    "\n",
    "\n",
    "a = 'lovely historic brownstone charm legal listing'\n",
    "b = '- lux astoria |of nyc| near subway/manhattan'\n",
    "c = ''\n",
    "d= 0.13124134134234\n",
    "\n",
    "str(d).isascii()\n",
    "\n",
    "\n",
    "\n",
    "#list(df_r_extra['name'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Density Map for Non-English Listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,30))\n",
    "sns_map_noneng = sns.scatterplot(x='longitude', y='latitude',s=20, data=df_noneng)\n",
    "ctx.add_basemap(sns_map_noneng, crs = 'EPSG:4326', source=ctx.providers.CartoDB.Positron)\n",
    "sns_map_noneng.set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(y = 'neighbourhood_group', data= df_noneng).set_title(\"Listings by Borough for Noneng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(y = 'neighbourhood', data= df_noneng).set_title(\"Listings by Borough for Noneng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_noneng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(y = 'host_name', data= df_noneng).set_title(\"Host Name for Noneng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(tidydf, x = 'price', title = 'Total Price Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df_noneng, x = 'price', title = 'Non eng Price Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_noneng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other research questions:\n",
    "\n",
    "Predicting Airbnb Prices with separate features and past data?\n",
    "\n",
    "Is there a way to find the most optimal listing for a trip?\n",
    "\n",
    "Want to buy a property using airbnb data which one will bring in the most profit depending on the existing data?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
